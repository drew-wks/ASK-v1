{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a xlsx of unqiue records from Qdrant without using the pdf_id field\n",
    "### This notebook is useful for gen 1 db where records have little to no metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "\n",
    "# CONFIG: qdrant\n",
    "api_key = st.secrets[\"QDRANT_API_KEY\"]\n",
    "url = st.secrets[\"QDRANT_URL\"]  # for cloud\n",
    "qdrant_collection_name = \"ASK_vectorstore\"\n",
    "# for local instance /private/tmp/local_qdrant\n",
    "qdrant_path = \"/Users/drew_wilkins/Drews_Files/Drew/Python/VSCode/ASK/data/qdrant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the collection and records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an instance of the client. WITHOUT LANGCHAIN\n",
    "# 22.5 sec for cloud\n",
    "# Running this locally places a lock file in the qdrant directory\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url,\n",
    "                      prefer_grpc=True,\n",
    "                      api_key=api_key,\n",
    "                      )\n",
    "\n",
    "'''usage'''\n",
    "content = (client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_record_dict():\n",
    "    all_records = client.scroll(\n",
    "        collection_name=qdrant_collection_name,\n",
    "        limit=100000,\n",
    "    )\n",
    "    records = all_records[0]\n",
    "\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    records_dict = {}\n",
    "\n",
    "    # Iterate through each record in the records list\n",
    "    for record in records:\n",
    "        # Extract the id and metadata from the record\n",
    "        record_id = record.id\n",
    "        metadata = record.payload['metadata']\n",
    "\n",
    "        # Exclude the 'page_content' from the metadata\n",
    "        if 'page_content' in record.payload:\n",
    "            del record.payload['page_content']\n",
    "\n",
    "        # Add the record to the records_dict\n",
    "        records_dict[record_id] = metadata\n",
    "\n",
    "    return records_dict\n",
    "\n",
    "\n",
    "all_records_dict = create_all_record_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for Duplicates  \n",
    "#### NOTE: In a few circumstances, you may have multiple chunks for one page, for example if the content is dense. This would throw a duplicate error below. In these cases, check the actual page content to detemine whether there is actually a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUPLICATE PDFs FOUND: 3\n",
      "\n",
      "./raw_pdfs/CH-2_TO_MSM VOLUME II_MARINE INDUSTRY PERSONNEL_CIM_16000_8B.pdf \n",
      "\n",
      "./raw_pdfs/USCG_Addendum_to_US_NSS_to_IAMSAR-CI_16130_2G-2022-10-01.pdf \n",
      "\n",
      "./raw_pdfs/Auxiliary_Operations_Process_Guide_Volume_I-General_Surface-16798_31A.pdf \n",
      "\n",
      "\n",
      "DUPLICATE RECORDS: 3 \n",
      "\n",
      "Original:\n",
      "id: 5a4f02c4-293d-4f36-b91f-71f36bdce143, source: ./raw_pdfs/CH-2_TO_MSM VOLUME II_MARINE INDUSTRY PERSONNEL_CIM_16000_8B.pdf, page: 433\n",
      "Duplicate:\n",
      "id: 8e5a7a48-55ba-4162-94e8-c2dd4599bbc9, source: ./raw_pdfs/CH-2_TO_MSM VOLUME II_MARINE INDUSTRY PERSONNEL_CIM_16000_8B.pdf, page: 433\n",
      "\n",
      "Original:\n",
      "id: 0e08ba75-ddfa-4d2c-a4a2-dea5ee85011b, source: ./raw_pdfs/Auxiliary_Operations_Process_Guide_Volume_I-General_Surface-16798_31A.pdf, page: 169\n",
      "Duplicate:\n",
      "id: ceb98d46-8234-4a7a-a3fc-c5af8340affb, source: ./raw_pdfs/Auxiliary_Operations_Process_Guide_Volume_I-General_Surface-16798_31A.pdf, page: 169\n",
      "\n",
      "Original:\n",
      "id: 2affb59b-456e-4385-8506-487da50ed27c, source: ./raw_pdfs/USCG_Addendum_to_US_NSS_to_IAMSAR-CI_16130_2G-2022-10-01.pdf, page: 109\n",
      "Duplicate:\n",
      "id: d8efda0f-be15-43c8-b018-78e6344a7425, source: ./raw_pdfs/USCG_Addendum_to_US_NSS_to_IAMSAR-CI_16130_2G-2022-10-01.pdf, page: 109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_pdfs_in_records(all_records_dict):\n",
    "    seen = {}  # Dictionary to keep track of source-page combinations\n",
    "    duplicate_record_count = 0  # Counter to track the number of duplicate records\n",
    "    duplicate_pdf_sources = set()  # Set to track unique sources of duplicate PDFs\n",
    "    duplicate_details = []  # List to store details of each duplicate pair\n",
    "\n",
    "    for record_id, record in all_records_dict.items():\n",
    "        source = record['source']\n",
    "        page = record['page']\n",
    "\n",
    "        # Create a tuple to represent the combination of source and page\n",
    "        key = (source, page)\n",
    "\n",
    "        if key in seen:\n",
    "            # Increment the duplicate record count\n",
    "            duplicate_record_count += 1\n",
    "\n",
    "            # Add the source to the set of duplicate PDFs\n",
    "            duplicate_pdf_sources.add(source)\n",
    "\n",
    "            # Store the original and duplicate records for detailed output later\n",
    "            original_record = seen[key]\n",
    "            duplicate_details.append({\n",
    "                \"original\": {\"id\": original_record[\"id\"], \"source\": original_record[\"source\"], \"page\": original_record[\"page\"]},\n",
    "                \"duplicate\": {\"id\": record_id, \"source\": source, \"page\": page}\n",
    "            })\n",
    "        else:\n",
    "            # Store this unique (source, page) combination along with the record ID\n",
    "            seen[key] = {\"id\": record_id, \"source\": source, \"page\": page}\n",
    "\n",
    "    # Calculate the number of unique PDFs that have duplicates\n",
    "    duplicate_pdf_count = len(duplicate_pdf_sources)\n",
    "\n",
    "    # Output the requested information\n",
    "    print(f\"DUPLICATE PDFs FOUND: {duplicate_pdf_count}\")\n",
    "    print(\"\")\n",
    "    for pdf_name in duplicate_pdf_sources:\n",
    "        print(pdf_name, \"\\n\")\n",
    "    print(\"\\nDUPLICATE RECORDS:\", duplicate_record_count, \"\\n\")\n",
    "\n",
    "    # Print each duplicate pair as specified\n",
    "    for detail in duplicate_details:\n",
    "        original = detail[\"original\"]\n",
    "        duplicate = detail[\"duplicate\"]\n",
    "        print(\"Original:\")\n",
    "        print(\n",
    "            f\"id: {original['id']}, source: {original['source']}, page: {original['page']}\")\n",
    "        print(\"Duplicate:\")\n",
    "        print(\n",
    "            f\"id: {duplicate['id']}, source: {duplicate['source']}, page: {duplicate['page']}\")\n",
    "        print(\"\")  # Blank line between duplicate pairs for readability\n",
    "\n",
    "\n",
    "# Example usage\n",
    "duplicates = find_duplicate_pdfs_in_records(all_records_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get unique records (list of PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDFs: 252\n"
     ]
    }
   ],
   "source": [
    "def create_unique_sources_dict(all_records_dict):\n",
    "\n",
    "    # Use a set to keep track of unique sources\n",
    "    seen_sources = set()\n",
    "\n",
    "    # Dictionary to store records with unique sources\n",
    "    unique_dict = {}\n",
    "\n",
    "    # Iterate through each record in the all_records_dict\n",
    "    for record_id, metadata in all_records_dict.items():\n",
    "        # If the record's source is not in the set, add it to the set and the unique_dict\n",
    "        source = metadata['source']\n",
    "        if source not in seen_sources:\n",
    "            seen_sources.add(source)\n",
    "            unique_dict[record_id] = metadata\n",
    "\n",
    "    return unique_dict\n",
    "\n",
    "\n",
    "'''usage'''\n",
    "unique_sources_dict = create_unique_sources_dict(all_records_dict)\n",
    "print(\"Number of PDFs:\", len(unique_sources_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Format the List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_unique_source_dict(unique_sources_dict):\n",
    "    '''Format values and store as a dataframe'''\n",
    "\n",
    "    formatted_list = []\n",
    "\n",
    "    for record_id, metadata in unique_sources_dict.items():\n",
    "        # Extract the base filename without the extension\n",
    "        source = metadata['source']\n",
    "        base_filename = os.path.splitext(os.path.basename(source))[0]\n",
    "        metadata['source_short'] = base_filename\n",
    "\n",
    "        # Append the metadata to the list\n",
    "        formatted_list.append(metadata)\n",
    "\n",
    "        # exclude all metadata from list except these (source will be removed once we have all the metadata\n",
    "        # metadata = {key: metadata[key] for key in ['short_source', 'source'] if key in metadata}\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    dataframe = pd.DataFrame(formatted_list)\n",
    "\n",
    "    # Remove the 'page' column\n",
    "    dataframe = dataframe.drop(columns=['page'])\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "'''usage'''\n",
    "unique_sources_df = format_unique_source_dict(unique_sources_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the List to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "def write_library_xlsx(unique_sources_df):\n",
    "    \"\"\"write dataframe to an Excel file.\"\"\"\n",
    "\n",
    "    # Get the current date and time in Zulu (UTC) time\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    timestamp = now_utc.strftime('%d%b%Y-%H%M')\n",
    "\n",
    "    # Specify the relative path to save the Excel file with the timestamp appended\n",
    "    file_path = f'qdrant_unique_records_report_{timestamp}.xlsx'\n",
    "\n",
    "    # Save DataFrame to Excel\n",
    "    unique_sources_df.to_excel(file_path, index=False)\n",
    "\n",
    "\n",
    "'''usage'''\n",
    "write_library_xlsx(unique_sources_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
